# -*- coding: utf-8 -*-
"""LVADSUSR_170_REGRESSION_LAB1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_zcc9xyeB1ciIkIYrveRFZYL4FESoH_z
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from scipy.stats import zscore
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, explained_variance_score, r2_score

import warnings
warnings.filterwarnings("ignore")

df=pd.read_csv('/content/Fare prediction.csv')

df.head()

df.info()

df.isnull().sum()

df.shape

df.duplicated().sum()

df.head()

for i in df.select_dtypes([int,float]):
  plt.boxplot(df[i])
  plt.title(f'boxplot of {i}')
  plt.ylabel("Frequency")
  plt.show()

numerical=df.select_dtypes([int,float])
q1=numerical.quantile(0.25)
q3=numerical.quantile(0.75)
iqr=q3-q1
lower=q1-1.5*iqr
upper=q3+1.5*iqr

outliers=df[df[numerical.columns]<lower]
outliers=outliers[outliers[numerical.columns]>upper]
df=df[~outliers.any(axis=1)]

for i in df.select_dtypes([int,float]):
  plt.hist(df[i])
  plt.title(f'Histogram of {i}')
  plt.ylabel("Frequency")
  plt.show()

for i in range(len(numerical.columns)):
  for j in range(i+1,len(numerical.columns)):
    plt.scatter(df[numerical.columns[i]],df[numerical.columns[j]])
    plt.title(f'Scatter plot of {numerical.columns[i]} and {numerical.columns[j]}')
    plt.show()

x=df.drop(['key','pickup_datetime','fare_amount'],axis=1)
y=df['fare_amount']

scaler=StandardScaler()
X_std=scaler.fit_transform(x)

X_train,X_test,y_train,y_test  = train_test_split(X_std,y,test_size=0.3,random_state=42)

model = RandomForestRegressor()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, color='blue', alpha=0.5, label='Actual vs. Predicted')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Ideal line')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs. Predicted')
plt.legend()
plt.show()

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mape = mean_absolute_percentage_error(y_test, y_pred)
explained_var = explained_variance_score(y_test, y_pred)
r2 = r2_score(y_test, y_pred)


print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Percentage Error:", mape)
print("Explained Variance Score:", explained_var)
print("R-squared:", r2)

metrics = {'Mean Squared Error': mse, 'Mean Absolute Error': mae, 'Root Mean Squared Error': rmse,
           'Mean Absolute Percentage Error': mape, 'Explained Variance Score': explained_var, 'R-squared': r2}
plt.figure(figsize=(12, 6))
plt.bar(metrics.keys(), metrics.values(), color=['blue', 'green', 'orange', 'red', 'purple', 'cyan'])
plt.xlabel('Metrics')
plt.ylabel('Value')
plt.title('Model Metrics')
plt.xticks(rotation=45)
plt.show()

